Naive Bayes classifiers are a set of supervised learning methods used for classification. They are based on the Bayes classifier with the naive assumption of conditional independence between features. The conditional independence simplifies the Bayes classifier to the following rule:
$$
C^{\text{Naive}}(x) = \underset{y}{\mathrm{argmax}}\,\mathrm{P}(y) \prod^n_{i=1}\mathrm{P}(x_i | y)
$$
The parameter $\mathrm{P}(y)$ is calculated as the relative frequency of class $y$ in the training set, while $\mathrm{P}(x_i|y)$ is estimated using the maximum a posteriori estimation. The different naive Bayes classifiers differ mainly by the assumptions they make about the probability distribution $\mathrm{P}(x_i|y)$.

Although the assumption of conditional independence is rarely true, naive Bayes classifiers work surprisingly well in many real-world problems, famously text classification and spam filtering. This happens because the attribute dependencies often distribute evenly or cancel out each other.

Naive Bayes classifiers can be extremely fast compared to more sophisticated methods, and require a small amount of training data to estimate the necessary parameters. The decoupling of the class conditional feature distributions eradicates problems stemming from the curse of dimensionality.

It should be noted that although naive Bayes is known as a decent classifier, it is known to be a bad estimator, so the probability outputs should not be trusted too much.

## Bayes classifier
Bayes classifier is the classifier that has the lowest risk of all classifiers using the same set of features. The Bayes classifier finds the class label $y$ which has the highest conditional probability given the evidence $x$.
$$
C^{\text{Bayes}}(x) = \underset{y}{\mathrm{argmax}}\,\mathrm{P}(y|x)
$$
where,
$$
\mathrm{P}(y|x) = \frac{\mathrm{P}(y) \mathrm{P}(x|y)}{\mathrm{P}(x)} \propto \mathrm{P}(y) \mathrm{P}(x|y)
$$
The Bayes classifier is optimal and its risk, called the Bayes error rate, is minimal. The excess risk of a classifier $C$ is defined as the difference between its risk and the Bayes error rate. A classifier is consistent if its excess risk converges to zero as the size of the training set tends to infinity.
$$
\text{Excess risk}(C) = \mathcal{R}(C) - \mathcal{R}(C^{\text{Bayes}})
$$

## Gaussian naive Bayes
The Gaussian naive Bayes classifier assumes that each variable $x_i$ is a real number representing the value of a feature $i$, and the likelihood of each feature is assumed to be a Gaussian distribution. The Gaussian naive Bayes relies on the mean $\mu_y$ and variance $\sigma_y^2$ of $x_i$ is in each class $y$.
$$
\mathrm{P}(x_i|y) = \underbrace{\frac{1}{\sqrt{2 \pi \sigma_y^2}} \exp \left(-\frac{(x_i - \mu_y)^2}{2 \sigma_y^2}\right)}_{\text{Gaussian density function}}
$$

## Multinomial naive Bayes
The multinomial naive Bayes classifier assumes that each variable $x_i$ is the count of a feature $i$, generated by a multinomial distribution. It is often used for text classification where the features are represented as word vector counts.
$$
\mathrm{P}(x_i|y) = \frac{N_{yi} + \alpha}{N_y + \alpha n}
$$
where $N_{yi} = \sum_{x \in T} x_i$ is the number of times feature $i$ appears in a sample of class $y$ in the training set $T$, and $N_y = \sum^n_{i=1} N_{yi}$ is the total count of all features for class $y$.

If a class and feature value never occur together, then the probability estimate will be zero, which can be problematic in further computations. To prevent this, a pseudocount can be added. When $\alpha = 1$, it is called Laplace smoothing, and when $\alpha < 1$, it is called Lidstone smoothing.

## Complement naive Bayes
The complement naive Bayes classifier is an adaptation of the standard multinomial naive Bayes algorithm that is particularly suited for imbalanced datasets. CNB usually outperforms MNB on text classification tasks, often by a considerable margin.

## Bernoulli naive Bayes
The Bernoulli naive Bayes classifier assumes that each variable $x_i$ is a binary value representing the presence or absence of a feature $i$. The likelihood of a feature given a class is described by the following rule, which explicitly penalizes the absence of a feature $i$, unlike MNB.
$$
\mathrm{P}(x_i|y) = \underbrace{\mathrm{P}(x_i = 1 | y)x_i}_{\text{presence of feature}} + \underbrace{(1 - \mathrm{P}(x_i = 1|y))(1-x_i)}_{\text{absence of feature}}
$$
In the case of text classification, word occurrence vectors, instead of word count vectors, can be used to train the model and use the classifier. The Bernoulli naive Bayes classifier performs better on some datasets, especially those with shorter documents.

## Categorical naive Bayes
The categorical naive Bayes classifier assumes that each variable $x_i$ is a category which can take on any value from the set of all categories available to the feature $i$. The categorical naive Bayes classifier estimates a categorical distribution for each feature $i$ conditioned on the class $y$.
$$
\mathrm{P}(x_i=t|y) = \frac{N_{yit} + \alpha}{N_y + \alpha n_i}
$$
where $N_{yit}$ is the number of times category $t$ appears in the samples $x_i$ which belong to class $y$, $N_y$ is the number of samples with class $y$, $\alpha$ is the smoothing parameter, and $n_i$ is the number of available categories to feature $i$.
